
# Project Workflow: From Data to Decision

This document provides a detailed, step-by-step explanation of the entire project pipeline, clarifying the role of each file and directory. It serves as a comprehensive guide to how the system works, from initial data processing to final analysis.

---

## 1. The Foundation: Simulation & Configuration

Everything starts with the world the agents will live in and the rules they will follow.

-   **`sumo/` directory:** This directory holds the blueprints for our simulated world.
    -   `test.nod.xml` & `test.edg.xml`: These are the raw definitions of the intersection (nodes) and the roads (edges).
    -   `test.net.xml`: This is the compiled network map generated by the SUMO `netconvert` tool from the node and edge files.
    -   `medium_traffic.rou.xml`: This file defines the traffic itselfâ€”the number of cars, their routes, and how frequently they appear.
    -   `*.sumocfg` files: These are the main scenario files. They bring together a network (`.net.xml`) and a route file (`.rou.xml`) to create a runnable simulation.
    -   `*.add.xml` files: These add extra components. We use them to define our induction loop detectors for measuring throughput and to specify the rigid program for the Fixed-Time controller.

-   **`src/config.py`:** This is the central control panel for all experiments. It contains the crucial hyperparameters for the reinforcement learning agents, such as the learning rate (LR), discount factor (GAMMA), and the parameters for the exploration-exploitation strategy (EPS_START, EPS_END, EPS_DECAY).

## 2. The "Proactive" Element: Forecasting Future Demand

A key feature of this thesis is that the agent is not just reactive; it's proactive. It uses a forecast of future traffic to inform its decisions.

-   **`data/foi_transcript.csv`**: This is the real-world historical data used as the basis for our traffic profiles.
-   **`src/forecasting.py`**: This script (currently using dummy data for this preliminary stage) is responsible for using the Prophet library to analyze a traffic profile and generate a minute-by-minute forecast of future vehicle demand. This forecast is what gives the agent its "proactive" knowledge.

## 3. The Environment: Bridging Python and SUMO

-   **`src/sumo_environment.py`**: This is one of the most critical files. It is a Python class that acts as the bridge between our AI agent and the SUMO simulator.
    -   It handles the low-level details of starting, stepping through, and closing the SUMO simulation using the `traci` library.
    -   Its `step()` method is the engine of the simulation. It takes an action from the agent (0 for STAY, 1 for SWITCH), applies it to the traffic light in SUMO, and runs the simulation forward in time.
    -   Its `get_state()` method gathers all the information the agent needs to make a decision. It combines **reactive** data (live queue lengths from SUMO) with **proactive** data (the future demand from the forecast file) into a single state vector.
    -   It calculates the `reward` for the agent at each step, which we have defined as the negative of the total vehicle waiting time. This simple signal is all the agent has to learn from.

## 4. The "Brains": The Reinforcement Learning Agents

These files define the architectures of the different learning algorithms we compared.

-   **`src/q_learning_agent.py`**: The baseline learning agent. It uses a simple Python dictionary (a "Q-table") to learn. It is simple but struggles with complex environments.
-   **`src/dqn_agent.py`**: The proposed agent for this thesis. It uses a PyTorch neural network to approximate the Q-table, allowing it to handle the complex, continuous state from the simulation far more effectively.
-   **`src/d3qn_agent.py`**: The advanced agent. It uses a more complex "dueling" neural network architecture that often leads to more stable and efficient learning.

## 5. The "Teacher": Training the Agents

-   **`src/trainer.py`**: This script orchestrates the entire learning process. It acts as the "teacher" for the agents.
    1.  It loads the desired agent (`--agent` flag) and the `SumoEnvironment`.
    2.  It runs a loop for many "episodes" (in our case, 150).
    3.  In each episode, it gets the state from the environment, asks the agent to choose an action, passes the action to the environment, and receives the next state and a reward.
    4.  It then tells the agent to **learn** from this `(state, action, reward, next_state)` experience. For the DQN agents, this involves storing the experience in a "Replay Memory" and using batches from this memory to train the neural network.
    5.  At the end of all episodes, it saves the agent's learned "brain" (`.pkl` or `.pth` file) to the `models/` directory.

## 6. The "Examiner": Evaluating Performance

-   **`src/runner.py`**: This script acts as the final exam for the trained agents.
    1.  It loads a fully trained model from the `models/` directory.
    2.  It runs a single, long evaluation episode (3600 steps) with **no learning or exploration** (epsilon=0). The agent only uses its existing knowledge.
    3.  At every step, it records the performance metrics, such as waiting time and the number of cars passing the detectors.
    4.  At the end of the run, it calculates the final average metrics and appends a new, timestamped row to the `results.csv` file.

## 7. The "Analyst": Generating the Final Report

-   **`src/generate_analysis.py`**: This is the final script in the pipeline.
    1.  It reads the `results.csv` file generated by the evaluation runs.
    2.  It performs all the ranking and weighting calculations for the Tradeoff and Sensitivity Analyses.
    3.  It generates the `analysis_report.html` file, which contains all the formatted tables required for your thesis chapter.

---

## Future To-Dos & Next Steps

With the preliminary analysis for Chapter 3 complete, the project can now move into Phase 2.

1.  **Archive Chapter 3 Results**: Create a new directory, `experiments/chapter3_preliminary/`, and move the contents of the `models/` directory, `results.csv`, and all generated reports (`.html`, `.md`) into it. This preserves a clean snapshot of this experiment.

2.  **Improve Model Training**: Re-train the DQN and D3QN agents for a significantly longer period (e.g., 500-1000 episodes) to see if a performance difference emerges between them. This would be a valuable point of discussion for your final thesis.

3.  **Implement Zero-Shot Evaluation**: This is the core task for the next phase.
    -   Create new, unseen intersection scenarios in the `sumo/` directory. These should have different topologies (e.g., a five-way intersection, asymmetric lanes) and different traffic flows.
    -   Use the existing `runner.py` script to evaluate your best-trained model (e.g., `dqn_agent.pth`) on these **new maps without any retraining**.
    -   Run the `generate_analysis.py` script on these new results.

4.  **Write Chapter 4: Zero-Shot Generalization**: The results from the zero-shot evaluation will form the basis of the next chapter. The analysis will focus on how well the agent's performance (e.g., its ability to reduce wait times) transfers from the environment it was trained on to new environments it has never seen before.
